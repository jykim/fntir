\documentclass[openany]{now} % creates the journal version
% \documentclass{now}  % creates the book pdf version

% a few definitions that are *not* needed in general:
%\newcommand{\ie}{\emph{i.e.}}
%\newcommand{\eg}{\emph{e.g.}}
%\newcommand{\etc}{\emph{etc}}
\newcommand{\now}{\textsc{now}}

\title{Offline Evaluation for Information Retrieval}

% THe following author list is tentative
\author{
Jin Young Kim \\
Microsoft \\
jink@microsoft.com
\and
Emine Yilmaz \\
University College London \\
emine.yilmaz@ucl.ac.uk
\and
Paul Thomas \\
Microsoft \\
pathom@microsoft.com
}


\begin{document}

% the following settings can be set or left blank at first
%\copyrightowner{N.~Parikh and S.~Boyd}
% \volume{1}
% \issue{3}
% \pubyear{2014}
% \isbn{978-0521833783}
% \doi{1234567890}
% \firstpage{23}
% \lastpage{94}

\frontmatter  % title page, contents, catalog information

\maketitle

\tableofcontents

\mainmatter

\begin{abstract}

\end{abstract}

\chapter{Introduction}
\label{c-intro} % a label for the chapter, to refer to it later

\section{Overview of Evaluation Paradigms in IR}

\begin{verbatim}
Online vs. Offline evaluation
- What is it? Why is it important? How is it used?
- How are they different?
 (advantages/disadvantages?)

Offline evaluation vs. Log study (Click Modeling)
- Label-based vs. Behavior-based
- Experimental control(?)

Offline evaluation vs. User study
- Focus: system-to-system evaluation vs. understanding interaction/user behavior
- Scale(?) / Richness(?)
- Blurred distinction recently

Online-Offline Hybrid approaches
- Log-based offline evaluation (i.e, click models)
- Collecting feedback directly from users (Kim et al.)
\end{verbatim}

\cite{INR-XYZ}

\section{Offline Evaluation for IR}

\begin{verbatim}
Traditional Approaches in Offline Evaluation
- Concept of relevance 
- Labels/Metrics based on Query-URLs

Components of Offline Evaluation
	-	Search Task (Query / context)
	-	Judging Method (Interface / rating scale) 
	-	Metric
	-	Experiment

=>	Test collections for offline evaluation (combining all the components)

\end{verbatim}

\cite{INR-009} \cite{borlund2003} \cite{cleverdon67}

\section{Recent Trends in Offline Evaluation}

\begin{verbatim}
Need for User-centric Evaluation
- Definition of User-centric
	- Aiming for user satisfaction
	- Evaluation based on models of user behavior
- Traditional metrics seem to not agree much with online signals, as well as each other
	o	Need methodologies to better estimate user satisfaction & behavior
- How to address this issue?
	o	Metric design
	o	Judgment design

% Malone et al?

Extending the realms of evaluation
-	Whole-page evaluation
-	Session-level evaluation
-	Desktop vs. Mobile / Typed vs. Spoken IR

New approaches
-	Online-Offline hybrid (Li & Kim)
-	Crowdsourcing / agile experimentation
-	???

\end{verbatim}

\chapter{Metrics}

\begin{verbatim}
-	Basic IR evaluation metrics 
-	Ranking-based metrics (Tau/TauAP)
-	Evaluation metrics that are based on explicit models of user behaviour
	o	ERR, EBU, GAP, Time-biased gain, etc.
	o	Alpha-NDCG, IA metrics, etc.
	o	RBP / INST (notion of residual)
-	Estimating the distribution of parameters/metric values based on user data
\end{verbatim}

\chapter{Judging Method}

\begin{verbatim}
-	SERP-level evaluation 
	o	Side by side / SASI
-	Session/Task-based evaluation
	o	User study for search experience
-	Effort based judgments 
-	Relevance vs. Usefulness-based evaluation 
\end{verbatim}

\cite{Thomas2006}
\cite{Chandar2013} \cite{Al-Maskari2007} \cite{Bailey2010} \cite{CarteretteBCD08}

\chapter{Crowdsourcing Judgment Collection}

\begin{verbatim}
Crowd judges are closer to the user
-	Different components (experiment, interface design, payment)
-	Reducing noise in judging
	o	Multiple judgments and majority voting, etc.
	o	Statistics to measure judge agreement/noise
-	Incentivising judges and how to make it more attractive (payment / I/F)
-	Design decisions that need to be tackled
	o	Trade-off between how many labels per item 
(fewer items with many labels versus more items with fewer labels)
\end{verbatim}

\cite{Megorskaya2015} \cite{Davtyan2015}
% Gabriella's work 

\chapter{Experiment Design and Analysis}

\begin{verbatim}
Power analysis
Sensitivity analysis
Informativeness (MaxEnt)
\end{verbatim}

% Diane's tutorial

\chapter{Evaluation Practices from Industry}

\begin{verbatim}
How are the companies doing?
-	Google / Bing
-	Netflix
-	Facebook
Common features
- Online + offline evaluation
Practical tips
\end{verbatim}

\cite{Gomez-Uribe2015}

\backmatter  % references

\bibliographystyle{plainnat}
\bibliography{ireval,eval_survey,judging_methods,crowdsourcing}

\end{document}
