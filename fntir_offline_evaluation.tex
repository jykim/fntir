\documentclass[openany]{now} % creates the journal version
% \documentclass{now}  % creates the book pdf version

% a few definitions that are *not* needed in general:
\newcommand{\ie}{\emph{i.e.}}
\newcommand{\eg}{\emph{e.g.}}
\newcommand{\etc}{\emph{etc}}
\newcommand{\now}{\textsc{now}}
\newcommand{\newpar}{\bigskip\noindent}

\title{Offline Evaluation for Information Retrieval}

% THe following author list is tentative
\author{
	Jin Young Kim \\
	Microsoft \\
	jink@microsoft.com
	\and
	Emine Yilmaz \\
	University College London \\
	emine.yilmaz@ucl.ac.uk
	\and
	Paul Thomas \\
	Microsoft \\
	pathom@microsoft.com
}


\begin{document}

% the following settings can be set or left blank at first
%\copyrightowner{N.~Parikh and S.~Boyd}
% \volume{1}
% \issue{3}
% \pubyear{2014}
% \isbn{978-0521833783}
% \doi{1234567890}
% \firstpage{23}
% \lastpage{94}

\frontmatter  % title page, contents, catalog information

\maketitle

\tableofcontents

\mainmatter

\begin{abstract}
Offline evaluation provides characterization of an information retrieval (IR) system based on human judgments without relying on actual users in real-world environment. Offline evaluation, notably test collection based evaluation, has been dominant approaches in IR evaluation. It is no exaggeration that shared evaluation efforts such as TREC has defined the IR research over the years. The reason for this success lies in the ability to compare retrieval systems in a reusable manner.

Recently, there has been several trends which necessitates the change in the role and method of offline evaluation. First and foremost, online search engines with large-scale user base has become commonplace, enabling online evaluation based on user behavior. Also, there are many endpoints for search beyond desktop web browser such as mobile phone and conversational agents, and the types of search results has diversified beyond the list of web documents to include other results types and direct answers. Finally, crowdsourcing has provided ways for human judgments of any kind to be collected at an large scale. The overall outcome of this trend is the advent of new IR evaluation paradigms which are more user-centric, diverse and agile.

This survey aims to provide an overview of recent research in IR evaluation pertaining to the trends above. We first introduce offline evaluation for IR, focusing on how it relates to other evaluation paradigms such as online evaluation. We also overview traditional offline evaluation for IR, and how recent trends have shaped the research so far. We then review research in offline evaluation mainly on three levels: human judgment, evaluation metric and experiment design. This organization will allow readers to follow recent developments in research from micro-level (human judgment) to macro-level (experiment). Finally, we discuss evaluation practices from industry, which has been a major driving force in research and development in IR.


\end{abstract}

\chapter{Introduction}
\label{c-intro}

In this chapter, we survey the area and lay conceptual foundation for the rest of the paper. We first provide an overview of different approaches to IR evaluation. We then focus on general overview of offline evaluation, explaining traditional approaches and recent trends. Finally, we introduce a conceptual framework and the outline for the rest of this paper. (15-20 pages)

\section{Evaluation Paradigms in IR}

\newpar
New Landscape in IR Evaluation Research

- User-centric view (understanding user)

- Online evaluation (industry)

- User studies

- More endpoints / models

- Agile experimentation (crowdsourcing)

\newpar
Online evaluation \cite{INR-XYZ} vs. Offline evaluation \cite{INR-009} 

- What is it? Why is it important? How is it used?

- How are they different?

\newpar
Offline evaluation vs. Counterfactual online evaluation \cite{chuklin2015click} \cite{Li:2015, li2010contextual}

- Label-based vs. Behavior-based

- Data from controlled experiments vs. Natural observations

\newpar
Offline evaluation vs. User study \cite{kelly2009methods}

- Focus: system-to-system evaluation vs. understanding interaction/user behavior

- Difference in scale and richness

- Blurred distinction recently
\cite{Bron:2013}
\cite{Liu:2014}
\cite{Shah:2011}

\newpar
The role of offline evaluation

- Evaluation at the early stage of development

- Experimental control and resolution

- Reusability across experiments

\section{Offline Evaluation for IR}
\subsection{Traditional Approaches in Offline Evaluation}

Conceptual Model

- Labels/Metrics based on Query-URLs

- Test collections 

- Concept of relevance 

History

- TREC and related evaluation venues \cite{INR-009}

\cite{borlund2003} \cite{cleverdon67} \cite{voor:trec05}

\subsection{Recent Trends in Offline Evaluation}

Need for User-centric Evaluation

- Definition: Aiming for user satisfaction, or other user-visible measure such as engagement or task completion (\cite{scholer13}) / Evaluation based on models of user behavior

- Traditional metrics seem to not agree much with user behavior/satisfaction
 \cite{Al-Maskari2007} 

- Cross-metric studies btw. online and offline evaluation \cite{radl:comp10}

\newpar
Need methodologies to better estimate user satisfaction and behavior

-	Metric design \cite{YilmazSCR10, CarteretteKY11, ChapelleMZG09}

-	Judgment design \cite{VermaY16, VermaYC16}

\newpar
Extending the realms of evaluation

-	Whole-page evaluation \cite{Zhou:2012}

-	Session-level evaluation \cite{KanoulasCCS11, CarteretteKHC14}

-	Desktop vs. Mobile / Typed vs. Spoken IR \cite{VermaYC16}

\newpar
Online-Offline Hybrid approaches

- Log-based offline evaluation \cite{Li:2015} \cite{li2010contextual}

- Collecting feedback directly from users \cite{Kim2016}

- Crowdsourcing / Agile Experiment

\section{General Framework for Offline Evaluation}

General Components of Offline Evaluation

-	Experiment

-	Search Task (Query / context)

-	Evaluation Metric

-	Judging Method (Interface / rating scale) 

\newpar
Organization of this paper: A pipeline for offline evaluation

- Select an audience (who you want to talk to: end users, accountants, sysadmins, advertisers). 

- Collect appropriate data: documents, tasks, queries, judgments. Much of this data may already exist; but Chapter~\ref{c-human-judgment} deals with gathering judgments, which need to be created for the purpose.

- Choose a metric based on your tasks and on likely user models. Aggregate judgments, if needed, to compute the metric. Chapter~\ref{c-metrics} considers this.

- Examine the metric to draw some conclusion. This is covered in Chapter~\ref{c-experiment-design}.

\chapter{Collecting Human Judgments}
\label{c-human-judgment}

The first step in offline evaluation is collecting human judgments. In this chapter, we describe various considerations in collecting high-quality labels from human judges at scale. We first discuss the method for collecting search tasks, followed by the design of a judging method. We then discuss the collection of judgments, which is an non-trivial task to perform at scale. (20-25 pages)

\section{Collecting Search Tasks}

Collect hypothetical search tasks

- Examples from user study papers

\newpar
Sample search tasks from existing system

- Which sampling methods to use? \cite{Baeza-Yates:2015}

\section{Designing a Judging Method}

Judging Unit: URL vs. SERP-level evaluation

- Preference Judgment  \cite{Chandar2013} \cite{CarteretteBCD08}

- Side by side \cite{Thomas2006} \cite{Kim:2013}

- SASI \cite{Bailey2010} 

\newpar
Judgment for Desktop vs. Mobile environment \cite{Verma:2016:CRMD}

\newpar
Judgment based on Query vs. Intent Description \cite{Yilmaz:2014:EID}

\newpar
Session/Task-based evaluation \cite{Moraveji:2011} \cite{Xu:2009}

\newpar
Effort based judgments \cite{Yilmaz:2014} \cite{Verma:2016:EBJ}

- Relevance vs. Usefulness-based evaluation 

\section{Collecting Judgments}

Choosing Judges: 

- Crowd vs. Expert \cite{Kazai:2013} \cite{Alonso20121053}

- Query owner vs. non-owners \cite{Chouldechova:2013}

\newpar
Reducing noise in judging: 

- Anchoring bias in judging \cite{Shokouhi:2015}

- Multiple judgments and majority voting, etc. \cite{Venanzi:2014}

\cite{aroyo2013measuring} \cite{aroyo2013crowd}

\newpar
Efficient judgment collection using Crowdsourcing

- Design decisions that need to be tackled  \cite{Blanco:2011} \cite{Kazai2012} \cite{Alonso2012} \cite{Alonso:2015} \cite{Scholer:2013} 

- Incentivising judges and how to make it more attractive (payment / I/F)
\cite{Megorskaya2015} \cite{Davtyan2015}  \cite{Rokicki:2014}  \cite{Eickhoff:2012}

\section{Open Issues}

- Collecting labels for contextual / personalized search results

- Collecting labels for whole SERP / non-document results

- Collecting labels for non-traditional endpoints (i.e., conversational agent)

\chapter{Designing Evaluation Metrics}
\label{c-metrics}

The second step in offline evaluation is designing a meaningful evaluation metric. This is essentially the question of how to combine labels to meaningful numbers. For traditional IR evaluation where the labels are collected at query-URL level, combining labels to a metric requires quite a few assumptions, or even a user model. In this chapter, we go over the various considerations of IR metric design, as well as the user models behind these metrics. (20-25 pages)

\section{Basic IR evaluation metrics}

- Metrics based on absolute judgments (e.g. \cite{cooper73selecting})

- Metrics based on preference-based judgments, including e.g.\ aggregated in-situ side-by-side \cite{Thomas2006}

- Ranking-based metrics (Tau/TauAP)

\section{Metrics based on simple aggregation of labels/qrels}

- P, R, AP, RR

\section{Models of behavior}

Evaluation metrics that are based on explicit models of user behavior

- The cascade model and variants

- ERR, EBU, GAP, Time-biased gain, etc.

- Weighted precision metrics such as RBP, INST; notion of residual

- Alpha-NDCG, IA metrics, etc.

- Cost-based/economic models and the prospects of metrics from these

- Session-level metrics \cite{kanoulas2011evaluating} \cite{JÃ¤rvelin2008}

\section{Model fitting}

Fit of metrics to models; estimating the distribution of parameters/metric values based on user data

\cite{CarteretteKY11}, \cite{Moffat2013}

\section{Open issues}

Open issues in behavior models and the corresponding metrics

- Whole-page quality

- Caption effects

- Variation between users: behaviors, learning styles, cognitive styles, topic expertise, search system expertise, expectations of the system, query variation, \dots

- Duplication in SERPs

- Learning (?)

- Non-traditional tasks


\chapter{Designing and Analyzing Experiments}
\label{c-experiment-design}

Experiments is defined as a set of labels and metrics defined on top of them. We first look over many considerations in order to design an experiment given a budget and time constraint. We then focus on a set of analyses we can perform once the data is collected.

\section{Designing an Experiment}

- How to select queries?

- How many queries? \cite{Sakai:2014}

- How many documents? \cite{CarterettePFK09}

- How to distribute judgment effort across queries and documents? \cite{CarterettePKAA09, YilmazR09}


\section{Analysis of Experimental Results}

Drawing conclusions from metrics (\ensuremath{\approx} 15 pages)

- Hypothesis Testing \cite{Dincer:2014}

- Comparison of different types of significance tests \cite{SmuckerAC09}

\newpar
Various analysis
- Power analysis \cite{Sakai:2014}

- Failure analysis

- Sensitivity and Reliability analysis \cite{Urbano:2013} 

- Informativeness (MaxEnt) \cite{AslamYP05}

- ETC \cite{Bron:2013} \cite{Boytsov:2013}  \cite{Robertson:2012}

\newpar
Reporting results

- Effect sizes and distributions, vs point estimates and $p$ values

\section{Open Issues}

\chapter{IR Evaluation in Practice}

In this chapter, we review evaluation practices happening in both academia and industry.

\section{Evaluation Practices from Academia}

- Dataset generation efforts (Living labs)


\newpar
Evaluation in related domains

- Aggregate search \cite{Zhou:2013}

- Recommendation systems \cite{gunawardana2015evaluating}

\section{Evaluation Practices from Industry}

How are the practitioners doing? (\ensuremath{\approx}15 pages)

-	Google \footnote{How Search Works (Google) https://www.google.com/insidesearch/howsearchworks/thestory/} \footnote{Updating Our Search Quality Rating Guidelines
	 https://webmasters.googleblog.com/2015/11/updating-our-search-quality-rating.html}

-	Bing \footnote{The Role of Content Quality in Bing Ranking (Bing)
	 http://bit.ly/1T1BaYN}

-	Netflix \cite{Gomez-Uribe2015}  \footnote{The Netflix Tech Blog: Learning a Personalized Homepage
	http://techblog.netflix.com/2015/04/learning-personalized-homepage.html}

-	Facebook \footnote{Who Controls Your Facebook Feed (Slate) http://slate.me/1T1BbvU}

\newpar
Common features

- Combine online and offline evaluation

- Offline evaluation for early iteration

- Online evaluation for final ship decision

\chapter{Conclusions}

Emerging trends in the tech ecosystem

Future of Offline Evaluation

\backmatter  % references

\bibliographystyle{plainnat}
\bibliography{ch1_intro,ch1_user_study,ch2_judgment,ch2_crowdsourcing,ch3_metrics,ch4_experiment,ch5_industry}
	
\end{document}
