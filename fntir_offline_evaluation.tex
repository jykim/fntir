\documentclass[openany]{now} % creates the journal version
% \documentclass{now}  % creates the book pdf version

% a few definitions that are *not* needed in general:
\newcommand{\ie}{\emph{i.e.}}
\newcommand{\eg}{\emph{e.g.}}
\newcommand{\etc}{\emph{etc}}
\newcommand{\now}{\textsc{now}}
\newcommand{\newpar}{\bigskip\noindent}

\title{Offline Evaluation for Information Retrieval}

% THe following author list is tentative
\author{
	Jin Young Kim \\
	Microsoft \\
	jink@microsoft.com
	\and
	Emine Yilmaz \\
	University College London \\
	emine.yilmaz@ucl.ac.uk
	\and
	Paul Thomas \\
	Microsoft \\
	pathom@microsoft.com
}


\begin{document}

% the following settings can be set or left blank at first
%\copyrightowner{N.~Parikh and S.~Boyd}
% \volume{1}
% \issue{3}
% \pubyear{2014}
% \isbn{978-0521833783}
% \doi{1234567890}
% \firstpage{23}
% \lastpage{94}

\frontmatter  % title page, contents, catalog information

\maketitle

\tableofcontents

\mainmatter

\begin{abstract}
Offline evaluation provides characterization of an IR system based on human judgments without relying on actual users in real-world environment. Offline evaluation, notably test collection based evaluation, has been dominant approaches in IR evaluation. It is no exaggeration that shared evaluation efforts such as TREC has defined the IR research over the years. The reason for this success lies in the ability to compare retrieval systems in a reusable manner.

Recently, there has been several trends which necessitates the change in the role and method of offline evaluation. First and foremost, online search engines with large-scale user base has become commonplace, enabling online evaluation based on user behavior. Also, there are many endpoints for search beyond desktop web browser, and the types of search results has diversified beyond the list of web documents to include other results types and direct answers. Finally, crowdsourcing has provided ways for human judgments of any kind to be collected at an large scale. The overall outcome of this trend is the advent of IR evaluation paradigm which is more user-centric, diverse and agile.

This survey aims to provide an overview of recent research in IR evaluation pertaining to the trends above. We first introduce offline evaluation for IR, focusing on how it relates to other evaluation paradigms such as online evaluation. We also overview traditional offline evaluation for IR, and how recent trends have shaped the research so far. We then review research in offline evaluation mainly on three levels: human judgment, evaluation metric and experiment design. This organization will allow readers to follow recent developments in research from micro-level (human judgment) to macro-level (experiment). Finally, we discuss evaluation practices from industry, which has been a major driving force in research and development in IR.


\end{abstract}

\chapter{Introduction}
\label{c-intro}

\section{Evaluation Paradigms in IR}

New Landscape in IR Evaluation Research

- More endpoints / models

- User-centric view (understanding user)

- Online evaluation (industry)

- Agile experimentation (crowdsourcing)

\newpar
Online \cite{INR-XYZ} vs. Offline evaluation \cite{INR-009} 

- What is it? Why is it important? How is it used?

- How are they different?


\newpar
Offline evaluation vs. Log study (Click Modeling) \cite{chuklin2015click}

- Label-based vs. Behavior-based

- Experimental control(?)

\newpar
Offline evaluation vs. User study \cite{kelly2009methods}

- Focus: system-to-system evaluation vs. understanding interaction/user behavior

- Scale(?) / Richness(?)

- Blurred distinction recently
\cite{Bron:2013}
\cite{Liu:2014}
\cite{Shah:2011}


\section{Offline Evaluation for IR}

Traditional Approaches in Offline Evaluation

- Concept of relevance 

- Labels/Metrics based on Query-URLs

- Test collections 

\cite{borlund2003} \cite{cleverdon67} \cite{voor:trec05}

\newpar
General Components of Offline Evaluation

-	Search Task (Query / context)

-	Judging Method (Interface / rating scale) 

-	Metric

-	Experiment

\section{Recent Trends in Offline Evaluation}



Definition of User-centric:

- Aiming for user satisfaction 

- Evaluation based on models of user behavior

\newpar
Need for User-centric Evaluation

- Cross-metric studies btw. online and offline evaluation \cite{radl:comp10}

- Traditional metrics seem to not agree much with user behavior/satisfaction
 \cite{Al-Maskari2007} 

\newpar
Need methodologies to better estimate user satisfaction and behavior

-	Metric design

-	Judgment design

% Malone et al?

\newpar
Extending the realms of evaluation

-	Whole-page evaluation

-	Session-level evaluation

-	Desktop vs. Mobile / Typed vs. Spoken IR

\newpar
Online-Offline Hybrid approaches

- Log-based offline evaluation \cite{Li:2015} \cite{li2010contextual}

- Collecting feedback directly from users (Kim et al.)

- Crowdsourcing / Agile Experiment

\chapter{Human Judgment}
Considerations in collecting high-quality labels from human judges at scale

\section{Judgment Design}

SERP-level evaluation: 

- Preference Judgment  \cite{Chandar2013} \cite{CarteretteBCD08}

- Side by side \cite{Thomas2006} \cite{Kim:2013}

- SASI \cite{Bailey2010} 

\newpar
Judgment for Desktop vs. Mobile environment \cite{Verma:2016:CRMD}

\newpar
Judgment based on Query vs. Intent Description \cite{Yilmaz:2014:EID}

\newpar
Session/Task-based evaluation \cite{Moraveji:2011} \cite{Xu:2009}

\newpar
Effort based judgments \cite{Yilmaz:2014} \cite{Verma:2016:EBJ}

- Relevance vs. Usefulness-based evaluation 

\section{Judgment Collection}

Choosing Judges: 

- Crowd vs. Expert \cite{Kazai:2013} \cite{Alonso20121053}

- Query owner vs. non-owners \cite{Chouldechova:2013}

\newpar
Reducing noise in judging: 

- Anchoring bias in judging \cite{Shokouhi:2015}

- Multiple judgments and majority voting, etc. \cite{Venanzi:2014}

\newpar
Efficient judgment collection using Crowdsourcing

-	Design decisions that need to be tackled  \cite{Blanco:2011} \cite{Kazai2012} \cite{Alonso2012} \cite{Alonso:2015} \cite{Scholer:2013} 

-	Incentivising judges and how to make it more attractive (payment / I/F)
\cite{Megorskaya2015} \cite{Davtyan2015}  \cite{Rokicki:2014}  \cite{Eickhoff:2012}

\chapter{Evaluation Metrics}
How to combine labels to meaningful numbers


Basic IR evaluation metrics

-	Ranking-based metrics (Tau/TauAP)

\newpar
Evaluation metrics that are based on explicit models of user behaviour

-	ERR, EBU, GAP, Time-biased gain, etc.

-	Alpha-NDCG, IA metrics, etc.

-	RBP / INST (notion of residual)

\newpar
Estimating the distribution of parameters/metric values based on user data

\newpar
Metrics for other domains of search

- Aggregate search \cite{Zhou:2013}
- Recommendation systems \cite{gunawardana2015evaluating}

\chapter{Experiment Design}
Drawing conclusions from metrics

- Hypothesis Testing \cite{Dincer:2014}

\newpar
Analysis of Results

- Power analysis \cite{Sakai:2014}

- Sensitivity and Reliability analysis \cite{Urbano:2013} 

- Informativeness (MaxEnt)

- ETC \cite{Bron:2013} \cite{Boytsov:2013}  \cite{Robertson:2012}

% Diane's tutorial


\chapter{Evaluation Practices from Industry}

How are the companies doing?

-	Google \footnote{How Search Works (Google) https://www.google.com/insidesearch/howsearchworks/thestory/}

-	Bing \footnote{The Role of Content Quality in Bing Ranking (Bing)
	 http://bit.ly/1T1BaYN}

-	Netflix \cite{Gomez-Uribe2015}

-	Facebook \footnote{Who Controls Your Facebook Feed (Slate) http://slate.me/1T1BbvU}

\newpar
Common features

- Online + offline evaluation

\newpar
Practical tips


\backmatter  % references

\bibliographystyle{plainnat}
\bibliography{ch1_intro,ch1_user_study,ch2_judgment,ch2_crowdsourcing,ch3_metrics,ch4_experiment,ch5_industry}
	
\end{document}
