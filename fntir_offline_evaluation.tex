\documentclass[openany]{now} % creates the journal version
% \documentclass{now}  % creates the book pdf version

\usepackage{subcaption}

% a few definitions that are *not* needed in general:
\newcommand{\ie}{\emph{i.e.}}
\newcommand{\eg}{\emph{e.g.}}
\newcommand{\etc}{\emph{etc}}
\newcommand{\now}{\textsc{now}}
\newcommand{\newpar}{\bigskip\noindent}

\usepackage{color}
\newcommand{\authornote}[3]{\marginpar{\tiny\color{#1}#2: #3}{\color{#1}{$\star$}}}
\newcommand{\jin}[1]{\authornote{red}{Jin}{#1}}
\newcommand{\emine}[1]{\authornote{green}{Emine}{#1}}
\newcommand{\paul}[1]{\authornote{blue}{Paul}{#1}}
\newcommand{\note}[1]{\textit{(#1)}}

\title{Offline Evaluation for Information Retrieval}

% THe following author list is tentative
\author{
	Jin Young Kim \\
	Microsoft \\
	jink@microsoft.com
	\and
	Emine Yilmaz \\
	University College London \\
	emine.yilmaz@ucl.ac.uk
	\and
	Paul Thomas \\
	Microsoft \\
	pathom@microsoft.com
}

\begin{document}

% the following settings can be set or left blank at first
%\copyrightowner{N.~Parikh and S.~Boyd}
% \volume{1}
% \issue{3}
% \pubyear{2014}
% \isbn{978-0521833783}
% \doi{1234567890}
% \firstpage{23}
% \lastpage{94}

\frontmatter  % title page, contents, catalog information

\maketitle

\tableofcontents

\mainmatter

\begin{abstract}
Offline evaluation characterizes an information retrieval (IR) system
% based on human judgments
without relying on actual users in a real-world environment. \paul{This suggests that lab studies are in scope.}\jin{I think it's hard to draw boundaries, except for its goals.} Offline evaluation, notably test collection based evaluation, has been the dominant approach in IR evaluation and it is no exaggeration to say that shared evaluation efforts such as the TREC conferences have defined IR research over the years. The reason for this success lies in the ability to compare retrieval systems in a reusable manner.

%Recently, there has been several trends which necessitates the change in the role and method of offline evaluation.
Several recent trends however necessitate a change in the role and methods of offline evaluation. First and foremost, online search engines with large-scale user base has become commonplace, enabling online evaluation based on user behavior \paul{Doesn't this suggest offline evaluation doesn't matter? Tone this down?}\jin{We'll talk about its limitations later}. There are new endpoints for search, such as mobile phones and conversational agents, and the types of search results has diversified beyond a list of web documents to include other result types. Finally, crowdsourcing has provided ways for human judgments of any kind to be collected at a large scale. However, such online evaluation based on user behavior has its own challenges due to repeatibility as well the extensive amount of time needed to get online evaluation signals from the users. Furthermore, most smaller companies and academic researchers do not have access to such large scale user base. Hence, recent research in IR evaluation has focused on the advent of new  offline evaluation paradigms which are more user-centric, diverse and agile.

This survey aims to provide an overview of recent research in IR evaluation pertaining to the trends above. We first introduce offline evaluation for IR, focusing on how it relates to other evaluation paradigms such as online evaluation. We also overview traditional offline evaluation for IR, and how recent trends have shaped the research so far. We then review research in offline evaluation on three levels: human judgments, evaluation metrics and experiment design. This organization will allow readers to follow recent developments in research from micro-level (human judgment) to macro-level (experiment). Finally, we discuss evaluation practice in industry, which has been a major driving force in research and development in IR.

%\emine{It may be better to have an organizaiton like:
%	* Importance of offline evaluation
%	* Difference from online evaluation and why it is important
%	* Components of offline evaluation
%	* Finally, organization of the paper}
% \paul{We more or less have this in \S\ref{sec:evaluation-paradigms}, I think, although probably each of the sections in Ch1 could be expanded}
\end{abstract}

% \emine{Maarten mentioned that since this will be like a book we should not have statements like recently (if I remember correctly)}
% \paul{Hmm---tricky. Isn't this whole effort motivated by recent changes? How should we describe them?}

\input{ch1_intro.tex}

\input{ch2_judgment.tex}

\input{ch3_metrics.tex}

\input{ch4_experiment.tex}

\input{ch5_industry.tex}

\input{ch6_conclusions.tex}

\backmatter  % references

\bibliographystyle{plainnat}
\bibliography{fntir}
	
\end{document}
