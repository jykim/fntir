\documentclass[openany]{now} % creates the journal version
% \documentclass{now}  % creates the book pdf version

\usepackage{subcaption}

% a few definitions that are *not* needed in general:
\newcommand{\ie}{\emph{i.e.}}
\newcommand{\eg}{\emph{e.g.}}
\newcommand{\etc}{\emph{etc}}
\newcommand{\now}{\textsc{now}}
\newcommand{\newpar}{\bigskip\noindent}

\usepackage{color}
\newcommand{\authornote}[3]{\marginpar{\tiny\color{#1}#2: #3}{\color{#1}{$\star$}}}
\newcommand{\jin}[1]{\authornote{red}{Jin}{#1}}
\newcommand{\emine}[1]{\authornote{green}{Emine}{#1}}
\newcommand{\paul}[1]{\authornote{blue}{Paul}{#1}}
\newcommand{\note}[1]{\textit{(#1)}}

\title{Offline Evaluation for Information Retrieval}

% THe following author list is tentative
\author{
	Jin Young Kim \\
	Microsoft \\
	jink@microsoft.com
	\and
	Emine Yilmaz \\
	University College London \\
	emine.yilmaz@ucl.ac.uk
	\and
	Paul Thomas \\
	Microsoft \\
	pathom@microsoft.com
}

\begin{document}

% the following settings can be set or left blank at first
%\copyrightowner{N.~Parikh and S.~Boyd}
% \volume{1}
% \issue{3}
% \pubyear{2014}
% \isbn{978-0521833783}
% \doi{1234567890}
% \firstpage{23}
% \lastpage{94}

\frontmatter  % title page, contents, catalog information

\maketitle

\tableofcontents

\mainmatter

\begin{abstract}
Offline evaluation characterizes an information retrieval (IR) system
% based on human judgments
without relying on actual users in a real-world environment. \paul{This suggests that lab studies are in scope.}\jin{I think it's hard to draw boundaries, except for its goals.} Offline evaluation, notably test collection based evaluation, has been the dominant approach in IR evaluation and it is no exaggeration to say that shared evaluation efforts such as the TREC conferences have defined IR research over the years. The reason for this success lies in the ability to compare retrieval systems in a reusable manner.

%Recently, there has been several trends which necessitates the change in the role and method of offline evaluation.
Several recent trends however necessitate a change in the role and methods of offline evaluation. First and foremost, online search engines with large-scale user base has become commonplace, enabling online evaluation based on user behavior \paul{Doesn't this suggest offline evaluation doesn't matter? Tone this down?}\jin{We'll talk about its limitations later}. There are new endpoints for search, such as mobile phones and conversational agents, and the types of search results has diversified beyond a list of web documents to include other result types. Finally, crowdsourcing has provided ways for human judgments of any kind to be collected at a large scale. However, such online evaluation based on user behavior has its own challenges due to repeatibility as well the extensive amount of time needed to get online evaluation signals from the users. Furthermore, most smaller companies and academic researchers do not have access to such large scale user base. Hence, recent research in IR evaluation has focused on the advent of new  offline evaluation paradigms which are more user-centric, diverse and agile.

This survey aims to provide an overview of recent research in IR evaluation pertaining to the trends above. We first introduce offline evaluation for IR, focusing on how it relates to other evaluation paradigms such as online evaluation. We also overview traditional offline evaluation for IR, and how recent trends have shaped the research so far. We then review research in offline evaluation on three levels: human judgments, evaluation metrics and experiment design. This organization will allow readers to follow recent developments in research from micro-level (human judgment) to macro-level (experiment). Finally, we discuss evaluation practice in industry, which has been a major driving force in research and development in IR.

%\emine{It may be better to have an organizaiton like:
%	* Importance of offline evaluation
%	* Difference from online evaluation and why it is important
%	* Components of offline evaluation
%	* Finally, organization of the paper}
% \paul{We more or less have this in \S\ref{sec:evaluation-paradigms}, I think, although probably each of the sections in Ch1 could be expanded}
\end{abstract}

% \emine{Maarten mentioned that since this will be like a book we should not have statements like recently (if I remember correctly)}
% \paul{Hmm---tricky. Isn't this whole effort motivated by recent changes? How should we describe them?}

\input{ch1_intro.tex}

\input{ch2_judgment.tex}

\chapter{Evaluation Metrics}
\label{c-metric}

The second step in offline evaluation is selecting or designing a meaningful evaluation metric. This is essentially the question of how to combine labels to meaningful numbers. For traditional IR evaluation where the labels are collected at query-URL level, combining labels to a metric requires quite a few assumptions, or even a user model. In this chapter, we go over the various considerations of IR metric design, as well as the user models behind these metrics. We briefly survey some established metrics but spend more time on recent developments: explicit models of user behavior, deriving metrics from these, and open issues including session-level measurement, dealing with variation, and considering rich SERPs. (20-25 pages)

\section{Basic IR evaluation metrics}

- Metrics based on absolute judgments (e.g. \cite{cooper73selecting})

- Metrics based on preference-based judgments, including e.g.\ aggregated in-situ side-by-side \cite{Thomas2006}

- Ranking-based metrics (Tau/TauAP)

- Criticisms: especially reproducability/replicability

\section{Metrics based on simple aggregation of labels/qrels}

- Set-based: P, R

- Rank-based: P@$k$, AP, RR

- Criticisms: what tasks and behaviors are modeled here?

\section{Models of behavior}

Evaluation metrics that are based on explicit models of user behavior

- The cascade model and variants

- Weights, the C/L/W framework \citep{Moffat2013}

- ERR, EBU, GAP, Time-biased gain, etc.

- Weighted precision metrics such as RBP, INST; notion of residual \citep{Moffat08,Moffat15}

- $\alpha$-NDCG, IA metrics, etc.

- Cost-based/economic models and the prospects of metrics from these

- Session-level metrics \cite{kanoulas2011evaluating} \cite{JÃ¤rvelin2008}

\section{Model fitting}

Fit of metrics to models; estimating the distribution of parameters/metric values based on user data

\cite{CarteretteKY11}, \cite{Moffat2013}

\section{Open issues}

Open issues in behavior models and the corresponding metrics

- Whole-page quality

- Caption effects

- Variation between users: behaviors, learning styles, cognitive styles, topic expertise, search system expertise, expectations of the system, query variation, \dots

- Duplication in SERPs

- Learning (?)

- Non-traditional tasks and novel UIs

- Choosing between metrics; sensitivity; finding evidence any of them correlates with user behavior or other important dependent variables

- Measuring things outside the SERP: query formulation, source/engine selection

\chapter{Test Collections}
\label{c-collection}

Experiments is defined as the collection of labels and metrics defined on top of them. We first look over many considerations in order to design an experiment given a budget and time constraint. We then focus on a set of analyses we can perform once the data is collected, followed by the ways of reporting experimental results. (\ensuremath{\approx} 15 pages)

\section{Designing an Experiment}

- How to select queries?

- How many queries? \cite{Sakai:2014}

- How many documents? \cite{CarterettePFK09}

- How to distribute judgment efforts across queries and documents? \cite{CarterettePKAA09, YilmazR09}


\section{Analysis of Experimental Results}

Survey of research results \cite{Sakai:2016}

Drawing conclusions from metrics 

- Hypothesis Testing \cite{Dincer:2014}

- Comparison of different types of significance tests \cite{SmuckerAC09}

\newpar
Various analysis methods

- Power analysis \cite{Sakai:2014}

- Failure analysis

- Sensitivity and Reliability analysis \cite{Urbano:2013} 

- Informativeness (MaxEnt) \cite{AslamYP05}

- ETC \cite{Bron:2013} \cite{Boytsov:2013}  \cite{Robertson:2012}

\newpar
Reporting results

- Effect sizes and distributions, vs point estimates and $p$ values

\section{Open Issues}

- Reusability for SERP/task-level evaluation

- Beyond significance testing -- bayesian alternatives?

- Reusability / Generalizability of experimental results


\chapter{IR Evaluation in Practice}
\label{c-practice}

In this chapter, we review evaluation practices happening in both academia and industry. We first cover evaluation practices from academia, including recent TREC tracks, data generation efforts. We also look at evaluation efforts in related area such as recommendation systems and conversational agents. We then turn to evaluation practices from industry including major players in search and recommendation based on published papers and articles.

\section{Evaluation Practices from Academia}

Emerging TREC tracks

- Task track

- Microblog track

- Live QA track

- Contextual suggestions track

\newpar
Dataset generation efforts

- Living labs for IR \footnote{http://living-labs.net/}

- Data set shared by industry \footnote{http://jeffhuang.com/search\_query\_logs.html}

\newpar
Evaluation in related domains

- Aggregate search \cite{Zhou:2013}

- Recommendation systems \cite{gunawardana2015evaluating}

- Conversational agents

\section{Evaluation Practices from Industry}

How are the practitioners doing? (\ensuremath{\approx}15 pages)

- Google \footnote{How Search Works (Google) https://www.google.com/insidesearch/howsearchworks/thestory/} \footnote{Updating Our Search Quality Rating Guidelines
	 https://webmasters.googleblog.com/2015/11/updating-our-search-quality-rating.html}

- Bing \footnote{The Role of Content Quality in Bing Ranking (Bing)
	 http://bit.ly/1T1BaYN}

- Netflix \cite{Gomez-Uribe2015}  \footnote{The Netflix Tech Blog: Learning a Personalized Homepage
	http://techblog.netflix.com/2015/04/learning-personalized-homepage.html}

- Facebook \footnote{Who Controls Your Facebook Feed (Slate) http://slate.me/1T1BbvU}

- Pinterest \footnote{Machine Learning at Pinterest http://www.slideshare.net/HiveData/the-hive-think-tank-machine-learning-at-pinterest-by-jure-leskovec-61383413}

- LinkedIn \footnote{http://www.slideshare.net/dtunkelang/search-quality-at-linkedin}

- Startups \footnote{The Humans Hiding Behind the Chatbots http://www.bloomberg.com/news/articles/2016-04-18/the-humans-hiding-behind-the-chatbots}

\footnote{10 Data Acquisition Strategies for Startups http://bit.ly/28IHlC7}

\newpar
Common features: combine online and offline evaluation

- Offline evaluation for early iteration

- Online evaluation for final ship decisions

\chapter{Conclusions}

In this chapter we conclude this survey by providing the summary of contents so far. 
We also provide a brief outlook toward the future of offline evaluation for IR.

\section{Summary}

Recap: general Components of Offline Evaluation

-	Experiment

-	Search Task (Query / context)

-	Evaluation Metric

-	Judging Method (Interface / rating scale) 


\section{Future of Offline Evaluation for IR}

Emerging trends in the tech ecosystem

- Mobile-first: different interfaces and information needs

- Natural-language interaction: Bots and Conversational agents

- End-to-end support for task completion: e.g., restaurant reservation 

\newpar
Future of Offline Evaluation

- Evaluation of search agents (as well as engines)

- Evaluation of various information 'cards'

- Evaluation of end-to-end task completion

\newpar
Future of Offline Evaluation Research

- Need for Academy-Industry collaboration

\backmatter  % references

\bibliographystyle{plainnat}
\bibliography{ch1_intro,ch1_user_study,ch2_judgment,ch2_crowdsourcing,ch3_metrics,ch4_experiment,ch5_industry}
	
\end{document}
