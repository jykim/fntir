
\chapter{Introduction}
\label{c-intro}

In this chapter, we survey the area and lay conceptual foundations for the rest of the paper. We first provide an overview of different approaches to IR evaluation. We then focus on offline evaluation, explaining traditional approaches and recent trends. Finally, we introduce a conceptual framework and the outline for the rest of this paper.

\section{Evaluation Paradigms in IR}
\label{sec:evaluation-paradigms}

Evaluating a search system, or any system that supports information access such as recommendation or filtering, is a complex problem. The performance of a search system is dependent on various contextual factors, such as the task at hand, the user's preference, abilities, location and other characteristics, and even the timing of the interaction. Also, the ultimate source of ground truth, the user's judgment, is subjective, volatile, and often hard to come by.

\subsection{Offline vs. Online Evaluation}

In order to meet these challenges, IR researchers have built a rich evaluation tradition. Most of this work has been based on a few simplifying assumptions. The document collection is static and the user's information need is represented as a description or a keyword query. The user's judgments in situ are replaced with judgments collected post-hoc and from third parties, often in the form of binary or numeric-scale labels.

We can define this evaluation paradigm as \textit{offline evaluation} \citep{INR-009} in that the evaluation of the system can happen without requiring an actual user. This makes offline evaluation particularly suitable for early-stage evaluation of an IR system, when users are hard to come by. Another typical characteristic of offline evaluation is that the test collection (a set of tasks, judgments and documents) is 'reusable', in that once built it can be used to evaluate new systems; because many factors are controlled, evaluations are also commensurable across time and between researchers.

An evaluation paradigm contrasting with offline evaluation is called \textit{online evaluation}. In a recent survey on this topic, online evaluation is defined as the evaluation of a fully functioning system based on measurement of real users' interactions with the system in a natural environment \citep{INR-XYZ}. That is, online evaluation directly employs user behavior in natural environment for evaluation.

As large-scale online services are commonplace now, online evaluation has become a viable option for companies with running services with large user bases. In the literature, there has been a plethora of papers on methodologies for online evaluation. While online evaluation has benefits in using data readily available as a by-product of serving users, this dependence on user behavior also creates limitations for online evaluation, which we will discuss later in this section.


%\subsection{Pros and Cons of Offline vs. Online Evaluation}

Now, let us compare two evaluation paradigms -- offline and online evaluation. 
Table~\ref{onlinevsoffline} summarizes the advantages and disadvantages of online vs. offline evaluation. %Now, when should we use online vs. offline evaluation? %While, we believe that these are complementary approaches , understanding relative pros and cons %While online metrics are certainly valuable and must-have when feasible, there are reasons we may need explicit input from human judges. 

\begin{table}
	\centering
	\footnotesize
	\caption{Pros and cons of offline vs. online evaluation}
	\begin{tabular}{|l|l|l|} \hline
 & Online & Offline \\ \hline
 Pros & Very little marginal cost & No need for production system \\
 & Based on actual user behavior & Easy to try new ideas \\
 &  & Amortized cost / reusability \\ \hline
 Cons & Need for production system & High marginal cost of label collection \\
 & Need for large-scale data & Need for judging infrastructure \\
 & Noisy interpretation of behavior & Need for `ground truth' judgments \\
 & & Difficult to model real users\ behavior \\
	\hline\end{tabular}
	\label{onlinevsoffline}
\end{table}

Offline evaluation typically requires access to explicit judgments of relevance obtained from relevance assessors, or judges. Obtaining judgments is an expensive procedure; hence, online evaluation tends to be cheaper compared to offline evaluation. Furthermore, online evaluation is based on signals that directly come from real users, which can enable us to get a more realistic signal of user satisfaction. 

On the other hand, online evaluation requires a running system as it is based on signals from real users. First, in initial stages of system development we simply might not have real users to study. Furthermore, small companies and academics may not have access to a large volume of users to be able to collect reliable signals. On the other hand, with the availability of various crowdsourcing services, it is relatively easy to collect labels from human judges.

Another major problem in online evaluation is that usually a significant amount of usage data is needed before one can reach reliable signals of satisfaction. Although there are techniques such as interleaving \citep{radl:comp10} which allows more sensitive online experimentation given the same amount of data, such technique is limited in the scope of evaluation. Hence, online evaluation tends to be very slow, which may not be suitable for evaluating the quality of new methodologies quickly. %On top of this, online evaluation necessitates the maintenance of a production system with a large user base along with experimental infrastructure, which is possible for large corporations but challenging otherwise.
\mdr{please qualify, see early work by Joachims}\jin{done}

More importantly, signals obtained from real users tend to be noisy and traces of user behavior are often insufficient to measure a user's true satisfaction. As an example, let's take clicks on results for evaluating a search engine. While a click is certainly an indication that the user is interested in the result, it is not clear whether the clicked result was actually satisfying \citep{Kim2016}. Also, clicks are often concentrated on the top of the page regardless of result quality \citep{radlinski2006minimally}, making them difficult to interpret. All in all, the ambiguity and bias inherent in user behavior often make it hard to infer users' real experiences. \mdr{What is "true quality"? In the eyes of the users? Of the judges? Of the managers?} \jin{I would say at the collective voice of end users, although capturing them would not be trivial.} \paul{have tried to clarify}

Another consideration is the reusability of the data collected. In offline evaluation, typically the label is collected at the level of individual information item (i.e., document) and the system is evaluated by its ability to put more relevant items on top. This means the labels can be reused to evaluate new systems that produce different rankings of the same items. By contrast, the data collected from online system is typically valid for the evaluation of the system user interacted with, although there are new research to address these issues.

Offline evaluation, on the other hand, tends to be fast once explicit judgments of relevance are obtained from relevance assessors. Once these judgments are collected, they can be used to evaluate the quality of sytems quickly. This makes offline evaluation very suitable for trying new ideas, and the initial cost can be amortised over many experiments. %Furthermore, offline evaluation mechanisms can be used to evaluate the quality of new systems; hence, they tend to be reusable and portable.

One major drawback of offline evaluation is the expense of collecting these explicit judgments, or the ground truth. Obtaining relevance judgments can be slow and expensive, and has to be repeated if the notion of relevance changes. Furthermore, these explicit judgments of relevance tends to come from a third-party assessor, as opposed to the real user of the system. Hence, the assessor may have a different understanding than an actual user as to what documents should be considered relevant \paul{This can make a difference---see Bailey et al., SIGIR'08, and responses}. Finally, depending on the domain (i.e., medical or engineering) or task (i.e., personalized search), it can be difficult to find capable assessors. 
\mdr{plus difficulties of getting expert labels in some domains/tasks (personalized search) plus dynamics of relevance}\jin{added}

Finally, offline evaluation metrics tend to be based on \emph{models} of user behavior, as opposed to behavior signals obtained from real users, and modeling users can be quite challenging due to the variance in behavior and expectations of real users. Hence, evaluation metrics based on user models may not necessarily reflect user satisfaction. Much recent work in offline evaluation focuses on this issue, which we will review in Chapter~\ref{c-human-judgment}.

\subsection{Hybrid Approaches}

So far we have compared two evaluation paradigms -- offline and online evaluation -- with distinctive characteristics. Offline evaluation is based on human judges as substitution of real users, and has strengths in experimental control and reusability. Online evaluation is based on user behavior, and has strengths in fidelity and cost.

% \paul{really? How about ``\dots is based on abstractions of real users''?}

While these two approaches comprise the majority of evaluation efforts, there have been several approaches trying to find a middle ground. Click modeling \citep{chuklin2015click} and counterfactual online evaluation \citep{Li:2015, li2010contextual}, for example, re-use online user data for future evaluation. These approaches, while enabling the re-use of online user data, are still limited in that they are based on implicit signals from user behavior. For instance, it is not trivial to decide whether a user indeed found the clicked document relevant or not, even with all the contextual information.

\mdr{Not sure. You want to make this argument. The exact same thing can/should be said about expert judges. They are not not real users so their judgments aren't necessarily an indication that users will find a document relevant/useful.}
\jin{The argument has been softened a bit}
%\paul{how is that a limitation? need to be explicit}\jin{I tried to explain}

Another related line of work is \textit{user study-based evaluation} \citep{Bron:2013, Liu:2014, Shah:2011}, which is widely used in interactive IR studies \citep{kelly2009methods}. In such work, a group of participants are typically brought into a lab environment and asked to perform a set of (usually predetermined) search tasks. It is common for this type of study to collect both behavior and labels from the participants to get a more complete picture of search activity. 

User studies bear similarities with offline evaluation in that they typically involve some form of explicit judgments, but their emphasis is more on understanding some aspect of users' search behavior, as opposed to comparative evaluation among search systems. Also, user studies tend to be limited in scale (typically less than 100 participants) and based on a biased, possibly not representative, sample of participants (typically people within the same institution).

\mdr{but then TREC is even worse off, just a handful of assessors...}
\jin{Crowdsourcing can help here...}

However, the distinctions are getting blurred as search engines increasingly serve more complex results, and SERP (search engine results page) or session-level evaluation is drawing more attention. In fact, some recent research has tried to employ task completion environment with human subjects for system-to-system comparison \citep{Xu:2009}. Also, crowdsourcing techniques are reducing barriers in getting access to a large number of subjects with diverse backgrounds. We will return to this point in Chapter~\ref{c-human-judgment}.

\subsection{Combining Approaches for Evaluation}
\jin{Paul/Emine -- please review this new section more carefully...}
Given a variety of options for evaluation -- online, offline and even hybrid ones, one may be confused to choose which one. However, the very existence of these different approaches predicate the multifaceted nature of the problem. Then, it makes sense to combine approaches for evaluation in order to get a full picture of the quality. Here we describe two approaches in combining multiple approaches in evaluation.  %In fact, there are vast range of IR problem with different properties and quality criteria.

%Given these advantages and disadvantages associated with online and offline evaluation, it is no wonder that most large scale companies tend to use a combination of both mechanisms, as we will go over in Chapter~\ref{c-practice}. Offline evaluation mechanisms tend to be used for quickly measuring the quality of new methods, and algorithms that show promising results are deployed to some selected subset of real users. Online evaluation mechanisms are then used to make the final decision about full-scale deployment. On the other hand, among smaller companies with limited users, and among academic researchers, offline evaluation remains the most viable mechanism.

\subsection{Funnel Approach for Combination}
In fact, it has been a common practice for IR evaluation in industry settings to combine offline and online evaluation in sequence. Since the number of algorithms approved for final deployment is many fewer than the algorithm sent for initial test, this whole process is sometimes called \textit{evaluation funnel}. 

The process starts with a set of new search algorithms which are candidates for deployment. First, they are evaluated against existing ranking technique (baseline) using an offline evaluation method. This ensures that the new techniques meet the minimal quality bar before they are exposed to users, and the cost of evaluation at this can be kept low if the labels can be reused across evaluations.

Once we have a subset of algorithms which passes the bar using offline evaluation, they are ready for online evaluation, which can be done by showing them to the user in a controlled experiment setting. This step ensures that the new algorithm does make positive user impact measured in the gain of various online metrics, which then would be `shipped' to the users. 

Now, for this funnel approach to work, it is important that the evaluation results earlier at the funnel (offline evaluation) should be show reasonable agreement with the results at the later stages (online evaluation). Ideally, the offline evaluation results should be a lenient filter which includes all the techniques with positive online evaluation results, so that it can reduce the number of algorithms sent for online evaluation.

For this reason, understanding the relationship between online and offline evaluation methods is important, and there has been several studies examining this issue. \cite{Huffman:2007} examines the relationship per-query relevance measures and session-level user satisfaction, finding that the relationship is quite strong, and that including the session-level information makes it even stronger. 
\mdr{The relation between online and offline evaluation needs a more thorough treatment. The discussion would have to include correlations between offline metrics and online metrics.}
\jin{Discussion added here}
%\citep{Al-Maskari2007,Sanderson2010} looked at the relationship between retrieval evaluation measures and user preference measures, showing reasonable correlation between two measures. 

\cite{Radlinski:2008} showed that paired experimental design for online evaluation (i.e., by interleaving two ranked lists) gives reasonable agreement with retrieval quality. \cite{radl:comp10} further develops this idea to compare the sensitivity between metrics, showing that offline evaluation based on 5,000 judges queries have sensitivity equivalent to 50,000 user impressions.

\subsection{User Modeling Approach for Combination}

`Funnel approach' above combines offline and online evaluation in sequence to find a set of ranking techniques that satisfies both. Alternatively, one can build a metric based on some model of user to capture the elusive concept of user satisfaction, and the combining data from online (user) and offline (judges) is often the key in building such user model.

For offline evaluation, online user behavior can inform the various parameters of the metric so that the resulting metric values better reflect user satisfaction. Recent work on offline evaluation metrics has embraced online user data to tune parameters of the metrics \citep[for example]{CarteretteKY11, Carterette:2012,smucker12stochastic,YilmazSCR10}.

For online evaluation, labels from human judges (or from user themselves, if available) can be used to build a model of successful search sessions. \citep{Hassan:2010,Hassan:2012} built a model of satisfaction based on human annotation of user sessions, or in-situ feedback from user, respectively. \cite{Ageev:2011} built a game-style interface where the goal is to perform a search task and then rate the experience, and then built a model of success which predicts the ratings given a sequence of behavior.

%Now, there are several conditions needed to make this funnel approach work. First, the evaluation metrics earlier at the funnel should be show reasonable agreement 

\subsection{Summary}
So far we have looked at various approaches in IR evaluation. Online evaluation based on user data and offline evaluation based on human judgments are two dominant approaches, and there many approaches which has the characteristics of both. Given these approaches with different characteristics, it makes sense to combine a few methods in many cases. In summary, here is our list of recommendations for deciding on which approach, or combination of approaches to use.

\begin{enumerate}
	\item Start with offline evaluation when usage data is not available, or the amount of user data is not sufficient to draw meaningful conclusions.
	\item Even if sufficient usage data is available, consider offline evaluation if you want to impose certain policy on evaluation results, or if there a certain area which online evaluation cannot cover.
	\item Consider user study if certain aspect of user behavior needs to be better understood, or detailed feedback of user experience is required.
	\item Consider combining both offline and online evaluation approaches in funnel, or building a user model which leverage both data sources.
\end{enumerate}

\section{General Framework for Offline Evaluation}

%In this section, we describe in detail a framework for offline evaluation. The goal is to propose a general framework which can encompass the diverse set of scenarios outlined above. \mdr{Vague: "diverse set of scenarios outlined above"}
%\subsection{Scenarios for Offline Evaluation}

We have outlined major evaluation paradigms for IR evaluation so far. The goal of this paper is to provide a practical guide to conducting offline evaluation for both academic and industry practitioners. Here we outline two major scenarios which we cover in this paper.

In traditional IR research, a typical evaluation scenario is to improve the performance of a document retrieval system given a test collection and a pre-determined set of evaluation metrics. For instance, in the TREC Web Track, participants are given a collection of web documents, and then asked to submit the results for their systems in a designated format. These are then evaluated on metrics like NDCG \citep{Jarvelin:2002} or ERR \citep{ChapelleMZG09}.

While academic IR research has developed well-accepted offline evaluation practices for document retrieval based on explicit labels, there are many evaluation scenarios not addressed in research from a practitioners' standpoint. There are multiple components in a modern IR system such as a web search engine, and designing evaluation for each requires different emphases and considerations. For instance, evaluating a query suggestion system can be quite different from evaluating a document ranking system. %, one can think of component-level (i.e., query suggestion) evaluation as opposed to system-level evaluation. 

Also, building a working system serving a large number of real users takes several stages of development. The evaluation at early stages of development would be more exploratory in nature, whereas at later stage the focus would shift to making ship decisions. We can call the former \textit{information-centric} evaluation in that the goal is to collect information helpful for system development and debugging, where the latter can be considered \textit{number-centric} in that the goal is to get reliable performance numbers for decision making.
%\paul{we don't use these terms again anywhere}

Another characteristics of IR evaluation in industry settings is that the evaluation is an on-going process which takes multiple iterations over the lifetime of the service, as opposed to a one-off research project. This necessitates the development of so called \textit{evaluation pipelines} where any new system can be evaluated on a ongoing basis.

Since the goal of this paper is to meet the need of practitioners as well as academic researchers, we describe decisions one needs to face in conducting offline evaluation across various scenarios outlined above. We also focus on considerations in designing a evaluation pipeline in industry settings at Chapter~\ref{c-practice}.

Dealing with evaluation problems across many scenarios requires a general framework of thinking. For the rest of this chapter, we introduce definitions and general process in offline evaluation which will constitute the framework.

\subsection{Definitions}

First, here are a few definitions that will be used throughout this paper. These comprise the components of offline evaluation.

\paragraph{Evaluation Goal} What is trying to be achieved through the evaluation? What is the coverage, criteria, and budget for evaluation?

\paragraph{Search Task}  A search task is the user's information needs. And it is typically represented as a description or as a query.

\paragraph{Search Engine} A search engine is an IR system that include both the interface and algorithm under the hood, and the goal of evaluation is to evaluate some aspect of search engine.

\paragraph{Judging Target} Judging target denotes a result produced by an IR system, and the item which is evaluated. It can be of any granularity -- a snippet, a web document, or entire SERP. 

\paragraph{Human Judgment} A human judgment is an assessment of a \textit{judging target} by a human judge, in the context of a \textit{search task}, over some dimension of quality. 

\paragraph{User Satisfaction} Measuring user satisfaction is considered as the goal of evaluation. It can be elicited directly from user, or be inferred by human judges given the transcript of search session.

\paragraph{User Model}  Abstraction of user behavior in a way that can be estimated with data with a goal to improve certain tasks. \jin{please verify}

\paragraph{Evaluation Metric} An evaluation metric (or metric in short) summarizes judgments into a single score. The design of an evaluation metric depends on the type of judgments being collected, and the model of user behavior.

\paragraph{Experiment} We define an experiment as a collection of search tasks, judging targets, and human judgments with a specific evaluation goal. An evaluation metric summarizes the outcome of an experiment with a test collection, and an appropriate statistical test can be used to make a claim about the validity and reliability of the findings.

\subsection{Evaluation Goal}
The first step in offline evaluation is defining a clear evaluation goal. Evaluation goal itself is a multifaceted concept which can include success criteria, coverage and criteria. These are all critical questions that can determine many parameters of evaluation. Here we look into each in detail.
\jin{Paul/Emine -- please review this new section more carefully...}

% Exploratory vs. Confirmatory
\paragraph{Evaluation Types}
First and foremost, evaluation goal should include the motivation and success criteria. While there can be many reasons for performing an evaluation -- understanding the performance, finding defects, making a shipping decision, etc., one can broadly define two types of evaluation -- exploratory vs. confirmatory, borrowing from the two major types of statistical analysis. %with different goals and emphases.

The goal of exploratory evaluation is to find the information about the performance of search engine in question. This can include assessing the performance of the search engine in overall, finding glaring defects to be fixed before shipping the product to the customers. In contrast, the goal of confirmatory evaluation is to derive numbers that can be a basis of decision making. This mostly takes the form of delta between two or multiple search engines. 

These two evaluation types take different criteria of success, which results in different approaches. For exploratory evaluation whose focus is the discovery, the design of judging interface should emphasize the collection of detailed information about the judging target, whereas for confirmatory evaluation the focus should be mostly on collecting accurate labels for metric calculation. 

\paragraph{Evaluation Coverage}
The coverage of evaluation denotes the target feature and the context. As for the target feature, a modern search engine is composed of various components such as query processor, ranking algorithm, and user interface generator, just to name a few. Therefore, it is important to clearly define which part of search engine to focus on.

Also, the effectiveness of a search engine is heavily dependent on various factors such as topic, user location and preference. For instance, a search engine which is excellent in sports and entertainment can be lousy in academic topics due to the type and characteristics of document collection. Therefore, it is crucial to design evaluation so that these various aspects are covered. 

\paragraph{Evaluation Criteria}
The quality of a search engine can be defined in many dimensions. Topical relevance would be a main concern for any search engine, but there are other criteria which can be the focus of evaluation, such as novelty, freshness and authority of results. Also, some evaluation criteria such as diversity and topic coverage can be defined at the granularity of results set.

\subsection{Evaluation Process}
Given the evaluation goal, here we discuss the general process for offline evaluation. At a high level, offline evaluation based on human judgment is composed of three steps: 1) judgment design, 2) metric design and 3) experiment design. Alternatively, you can consider the whole process in terms of collecting data (judgments), combining them into meaningful numbers (metrics), carry out experiments to test hypotheses and draw conclusions (test collection). Now we discuss major considerations in each step.

%\paul{surely you need a collection, or at least 2/3 of it -- tasks and docs -- to get the judgements in the first place. Perhaps ``\dots, carry out experiments to test hypotheses and draw conclusions''?}

%\begin{figure}
%	\begin{center}
%		\includegraphics[scale=0.4]{images/offline_evaluation_overview}
%		\caption{Overview of Offline Evaluation.} 
%		\label{fig:offline_evaluation_overview}
%	\end{center}
%\end{figure}
%\emine{This figure shows as though the metric is part of the test collection so we might need to change that}
\subsubsection{Designing Human Judgments}

In the first step, the details of human judgment should be defined, which is the basic unit of offline evaluation. Human judgments capture the quality of the results for given search tasks. Here are major considerations in this step:

\begin{enumerate}
	\item How do you define and collect search tasks?
	\item What should be your judging unit?
	\item How do you design judging interface?
	\item How do you hire and manage judges?%\paul{we don't cover training in ch2}
\end{enumerate}

\subsubsection{Designing Evaluation Metrics}

The second step in offline evaluation is selecting or designing a  evaluation metric. Metrics summarize the information from individual labels into meaningful numbers. This is essentially the question of how to combine labels to meaningful numbers.

\begin{enumerate}
	\item How do you transform the labels from human judges?
	\item How do you define user models in combining labels into a metric? 
	\mdr{You have not told us how or why user models enter the picture. Or what they are.}
	\jin{Definition for user model added}
	\item How do you estimate the parameters for the user model?
\end{enumerate}

\subsubsection{Designing Experiments}
Lastly, judgments and metrics should be combined to achieve the goal of evaluation. Since this is an iterative step which takes several stages of refinement, here we describe methods and criteria in doing so. 

%\paul{was ``designing test collections'', please check you're happy with the re-wording}
%\jin{Makes sense. ``Test collection'' seems a bit narrower.}

\begin{enumerate}
	\item How do you size the test collection to fulfill your evaluation goal?
	\item How do you evaluate the validity of the outcome?
\end{enumerate}

\subsection{Summary}
In this section, we introduced definitions and general process in offline evaluation which will constitute a general framework of offline evaluation. In what follows, we make the discussion more concrete by describing the trends in offline evaluation using this framework.

\section{Trends in Offline Evaluation for IR}

Information retrieval has a rich tradition of evaluation, both online and offline, and this tradition has been responsible for some of the rapid advances in search technology of the past two decades~\citep{TRECimpact}. Below we survey traditional approaches to offline evaluation, and consider trends in recent years which suggest new roles and methods.

\subsection{Traditional Approaches to Offline Evaluation}
\jin{Paul, please review and revise; definitions can be moved to earlier sections}
The traditional offline approach to IR evaluation is the test collection, or ``Cranfield'', approach first described by \cite{cleverdon67} and refined through exercises such as the Text REtrieval Conference (TREC; see \cite{voor:trec05} for an overview).  We will summarise this approach here, noting that \cite{INR-009} provides a historical summary and comprehensive discussion.

\begin{figure}
	%\centering
	{
	\includegraphics[width=0.67\textwidth]{images/ir-model}
	\subcaption{A simplified model of a retrieval system in context.}
	\label{fig:ir-simple}
	}\vspace*{3em}
	{
	\includegraphics[width=\textwidth]{images/ir-model-with-judges}
	\subcaption{Extension of the model, illustrating third-party relevance judges and the formation of a test collection.}
	\label{fig:ir-with-judges}
	}\vspace*{3em}
	\caption{Test-collection-based evaluation of an information retrieval system.}
\end{figure}
\mdr{That's a narrow view of IR: why not "provide the right information to the right person in the right way". So this could be about documents, but it could also be about answers, entities, etc.}\jin{done}
In its most basic form, we can think of an information retrieval system as providing the right information to the right person in the right way. A user has an information need; they express this as a query; and the system will draw on the collection of information to produce some set of results (Figure~\ref{fig:ir-simple}).

The test collection approach simulates this model by using the judgments of relevance and evaluation metrics that aim at measuring the quality of the results presented to the user  (Figure~\ref{fig:ir-with-judges}). The query, document collection, and retrieval system are as before but three components are added:

\mdr{Should you not more explicitly and formally define what these core concepts are: need, query, inut for a need, test collection plus the relation between them?}

\begin{description}
	\item[Judges] interpret the user's information need, for example on the basis of the query or other context; and consider the extent to which each document in the collection answers this need.
	
	\item[Judgments] record this information obtained from the judges for each (query, document) pair.
	
	\item[Evaluation] is then a matter of aggregating the recorded judgements for the set (or ranking) of documents retrieved by the system; or comparing the documents retrieved with the documents judged as relevant.
\end{description}

For example, precision can be calculated by counting the number of retrieved documents which were marked as relevant; recall can be calculated by comparing the number of retrieved documents judged relevant with the overall number judged relevant; or rank-sensitive metrics, such as average precision or reciprocal rank, can look at the judgment for each retrieved document in turn.

\subsubsection{Abstractions}
\mdr{should you not put that in the big picture from the start? in a sense, we are sampling everywhere, queries, documents, judges. it would be good to point out the uncertainties that come with this from the start}
In principle, the judgements formed are complete---that is, the collection includes a judgment for each possible (query, document) pair. Although this was true of some of the earliest exercises \citep{cleverdon66}, it is clearly impossible for today's much larger document sets. Two shortcuts have allowed researchers to collect useful judgments regardless.  First, a simplifying assumption is that every search is over the same, fixed, document collection: that is, the documents do not change over time and nor are they different for different information needs (or users---as would be the case for personal or corporate collections).

Even allowing for this, it is clearly impossible to judge each of millions (or billions) of documents for any arbitrary information need. \emph{Pooling} provides a common shortcut. \cite{} In a pooled evaluation, each of a number of search systems provides its own ranking of documents, perhaps by running the same query on each. Every document which appears in the top~$k$ in any ranking is then judged, so if $N$~systems contribute to the pool there are at most $Nk$~judgments to be made: likely fewer, as some documents will appear in more than one list.

A related assumption is that each need can be captured in a single expression: that is, most (although not all) collections include a single query for each need.  Although this is clearly a very small sample of possible inputs for the need, and although relatively small changes to a query can result in large changes in measured effectiveness~\citep{bailey15user}, a large sample of needs can still capture useful variation.

Judging is also abstracted from real users in real contexts, in order to collect judgments at scale. The largest assumption here---indeed, an assumption relied upon by most effectiveness metrics---is that judgments are independent. That is, it is assumed that the extent to which a document is relevant to a need is independent of any other document which might be returned, or the order in which they are seen by the user. Notable exceptions are techniques from \cite{Golbus:2014:CDR} and \cite{Chandar2013}, who used relevance judgments based on other documents.

The notion of ``relevance'' is also normally abstracted. Although in reality relevance is complex, multi-facteted, and highly contextual \citep{borlund2003,saracevic16relevance}, judges are often given much simpler instructions which can for example boil down to what \cite{borlund2003} calls ``intellectual topicality'' alone. Recent work such as that by \cite{Mao:2016} and \cite{VermaYC16} also aims at extending this narrow definition of relevance. We delve into this in Chapter~\ref{c-human-judgment}.

\subsubsection{Test Collections}

A central concept in traditional IR evaluation is the \emph{test collection}. A test collection is the combination of 

\begin{itemize}
	\item a fixed set of documents; 
	\item a set of information needs or topics, typically each with an associated query; and 
	\item a set of relevance judgments which detail the relevance of at least some documents to each need.
\end{itemize}

Because they involve a static representation of an information-seeking session, test collections can be distributed;\mdr{what does that mean} the judgments therein are reusable; and, in combination with one or more effectiveness metrics, they make it simple to compare systems.
\mdr{Explain how your survey relates to Mark Sanderson’s survey and why we need your survey now.}
%\subsubsection{Building Collections}
%
%\paul{to do?} glossed over the many questions of how to build these in practice. simulated work tasks. query variation. pooling choices. labelling choices, judge selection (g/s/b). etc. see\dots

%\subsubsection{Shared Evaluation Exercises}

Test collections have been especially valuable for evaluation as they are easy to re-use: typically the limiting factor is just physical (or network) distribution of the documents themselves.\mdr{nope: pool bias} Since they are so abstracted, they are self-contained, and it is trivial to compare results across systems, times, or laboratories.

A noteworthy example of this is the Text REtrieval Conference (TREC) series, run annually by the (U.S.) National Institute of Standards and Technology (NIST). Since 1992 these conferences have been based around shared evaluations, using test collections so that each participating system can be directly compared to others~\cite{voor:trec05}. The model has been adopted by a number of other conferences including the NTCIR Workshop\footnote{\url{http://research.nii.ac.jp/ntcir/index-en.html}}, the Conference and Labs of the Evaluation Forum (CLEF)\footnote{\url{http://www.clef-initiative.eu/}}, and the Forum for Information Retrieval Evaluation (FIRE)\footnote{\url{http://fire.irsi.res.in/}}. These collections now include genera such as the web, microblogs, genomics, tourism, email, and others and in virtue of their scope and portability have become standard tools for information retrieval research.\mdr{Think of Hersh paper about relation between offline and user studies. Think also of relation between offline and online: often online is the key metric (REF?); e.g., work by Aleksandr Chuklin.}

\subsection{Recent Trends in Offline Evaluation}
\jin{Emine, please review and revise; definitions can be moved to earlier sections}

So far we have looked at traditional approaches in IR evaluation. While this tradition has served the community well for the past few decades, there has been several trends which necessitate a change in the roles and methods of IR evaluation. In this section, we outline recent trends and discuss implications for offline evaluation.

\subsubsection{User-Centric Evaluation}
First and foremost, online search engines with large-scale user bases have become widely available and used, enabling online evaluation based on user behavior. This availability of user data has opened up the possibility of validating the assumptions of offline evaluation with actual user data. Recent work on evaluation metrics has embraced online user data to tune parameters of the metrics \citep[for example]{CarteretteKY11, Carterette:2012,smucker12stochastic,YilmazSCR10}.

The overall outcome of this trend is the advent of new IR evaluation paradigms which are more user-centric, diverse and agile. Here, being user-centric means that the evaluation process is based on a model of user behavior, or/and aims to improve user satisfaction or other user-visible measure such as engagement or task completion \citep{scholer13}. \mdr{this is a very diffuse definition of "user-centric, diverse and agile" evaluation. Can you split out the three notions and be more precise about the definitions of each? Also, what is a user-visible measure? Please define.}

This has already led to new methodologies to better estimate user satisfaction and behavior in judgment collection \citep{VermaY16, VermaYC16} or metric design \citep{YilmazSCR10, CarteretteKY11, ChapelleMZG09}. Also, some recent work has looked at cross-metric correlation, aiming to align IR evaluation with user satisfaction or some proxy of it \citep{Al-Maskari2007,radl:comp10}.\mdr{There is quite a bit more on this.}

%As a side note, there has been an increasing efforts to combine online and offline evaluation. These include ways to use online user data for offline evaluation \cite{Li:2015} \cite{li2010contextual} \cite{chuklin2015click}, or ways to collect feedback directly from user \cite{Kim2016}. 
%\note{mentions of user study / iir papers}

\subsubsection{Diverse Endpoints and Search Scenarios}

There are also new endpoints for search beyond desktop web browsers, such as mobile phones and conversational agents. This has opened up a whole area of research which focuses on different interaction methods and user experiences across endpoints. For instance, mobile devices have much smaller screen dimensions and the interaction is based on touch, while conversational agents use natural language, often in voice, to interact with the user.

Even for web search itself, the types of search results have diversified beyond the list of web documents to include other result types such as images, videos, news and even direct answers. This diverse set of results types, and corresponding user interface designs, breaks many assumptions of traditional IR evaluation, providing rich opportunities for exploration. In particular, many of these 'answers' can directly satisfy users' information needs on the SERP, making it hard to apply click-based evaluation techniques \citep{Li2009GA,diriye2012leaving}.\mdr{See Chuklin et al, CIKM 2016}
\mdr{I suggest organizing this differently: in every paragraph, first mention problem/challenge, then mention recent work that addresses this challenge, then scope: either point to later in the survey  in case you are addressing the problem or explicitly say that you are not addressing it}

IR evaluation research has with various lines of work. There has been increased interest in whole-page evaluation and optimization \citep{Zhou:2012}, which encompasses a wide variety of page elements beyond web results. %Side-by-side evaluation 
%
Task and session-level evaluation has also drawn interest \citep{KanoulasCCS11, CarteretteKHC14}, with TREC tracks of the same name \citep{carterette2014overview}. Finally, there have been new lines of work focusing specifically on mobile interfaces \citep{VermaYC16}, or evaluation of search with spoken agents \citep{Kiseleva:2016}.

\subsubsection{Crowdsourcing / Agile Evaluation}
\mdr{I don't see how the novelty or exploratory nature of new endpoints and scenarios calls for an agile manner of collecting labels. Vague.
	What do you mean "with less investments". Simply that it should be cheaper? 
	Or that TREC style judging does not scale for financial reasons?
	Aren't new devices and crowdsourcing orthogonal?}

These diverse new endpoints and scenarios for search required ways to collect labels in a more agile manner, because many of these services are new and exploratory by nature, with less investments compared to well-established ones like web search. Also, in academic settings, it has been difficult to recruit participants with diverse backgrounds at scale.

Fortunately, services such as Amazon Mechanical Turk have provided new ways for human judgments of any kind to be collected at a large scale. These services are called `crowdsourcing', in that they pull the `wisdom of the crowd' for tasks needing human intelligence. Accompanying this alternative data collection method is a challenge in quality control, since the labeling work is completed by a remote worker on the internet.

Given these opportunities and challenges, there has been quite a good deal of research on collecting high-quality labels with low effort \citep{Alonso2012}. Popular approaches include using overlapping judgments to identify ground truth labels \citep{Venanzi:2014}, or identifying the quality of judges based on their behaviors \citep{Kazai:2016}. We cover some of these methods in Section \ref{s-crowdsourcing}.

%\emine{Should we define that crowdsourcing is and how it may be useful for offline evaluation?}

\subsection{Summary}


\section{The Organization of this Survey}

In the following chapters, we describe each process of offline evaluation in detail so that a reader can design his or her own evaluation pipeline following the flow of this paper. Chapter~\ref{c-human-judgment} deals with gathering judgments, which need to be created for the purpose. Chapter~\ref{c-metric} considers steps in designing an effective metric. Chapter~\ref{c-collection} covers the methods in designing and analyzing experiments. Finally, Chapter~\ref{c-practice} describes evaluation practices from major companies in search and recommendation area.
\mdr{How is this consistent with earlier statements and definitions that all seem to focus on *document retrieval*,}\jin{We'll discuss IR evaluation more generally.}

%\emine{We already had a part describing the organization. In general, this section feels a bit repetitive given the text in first page}
%\paul{I disagree, that was in the abstract; it makes sense here (as well) I think. Unless I missed something?}