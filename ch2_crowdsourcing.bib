@inproceedings{Venanzi:2014,
	author = {Venanzi, Matteo and Guiver, John and Kazai, Gabriella and Kohli, Pushmeet and Shokouhi, Milad},
	title = {Community-based Bayesian Aggregation Models for Crowdsourcing},
	booktitle = {Proceedings of the 23rd International Conference on World Wide Web},
	series = {WWW '14},
	year = {2014},
	isbn = {978-1-4503-2744-2},
	location = {Seoul, Korea},
	pages = {155--164},
	numpages = {10},
	url = {http://doi.acm.org/10.1145/2566486.2567989},
	doi = {10.1145/2566486.2567989},
	acmid = {2567989},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {bayesian inference, community detection, crowdsourcing},
} 

@Article{Kazai2012,
	author="Kazai, Gabriella
	and Kamps, Jaap
	and Milic-Frayling, Natasa",
	title="An analysis of human factors and label accuracy in crowdsourcing relevance judgments",
	journal="Information Retrieval",
	year="2012",
	volume="16",
	number="2",
	pages="138--178",
	issn="1573-7659",
	doi="10.1007/s10791-012-9205-0",
	url="http://dx.doi.org/10.1007/s10791-012-9205-0"
}

@Article{Alonso2012,
	author="Alonso, Omar",
	title="Implementing crowdsourcing-based relevance experimentation: an industrial perspective",
	journal="Information Retrieval",
	year="2012",
	volume="16",
	number="2",
	pages="101--120",
	issn="1573-7659",
	doi="10.1007/s10791-012-9204-1",
	url="http://dx.doi.org/10.1007/s10791-012-9204-1"
}

@article{Alonso20121053,
	title = "Using crowdsourcing for \{TREC\} relevance assessment ",
	journal = "Information Processing \{&\} Management ",
	volume = "48",
	number = "6",
	pages = "1053 - 1066",
	year = "2012",
	note = "",
	issn = "0306-4573",
	doi = "http://dx.doi.org/10.1016/j.ipm.2012.01.004",
	url = "http://www.sciencedirect.com/science/article/pii/S0306457312000052",
	author = "Omar Alonso and Stefano Mizzaro",
	keywords = "\{IR\} evaluation",
	keywords = "Test collections",
	keywords = "Relevance assessment",
	keywords = "Crowdsourcing",
	keywords = "TREC",
	keywords = "Amazon Mechanical Turk",
	keywords = "Experimental design "
}

@inproceedings{Alonso:2015,
	author = {Alonso, Omar and Marshall, Catherine C. and Najork, Marc},
	title = {Debugging a Crowdsourced Task with Low Inter-Rater Agreement},
	booktitle = {Proceedings of the 15th ACM/IEEE-CS Joint Conference on Digital Libraries},
	series = {JCDL '15},
	year = {2015},
	isbn = {978-1-4503-3594-2},
	location = {Knoxville, Tennessee, USA},
	pages = {101--110},
	numpages = {10},
	url = {http://doi.acm.org/10.1145/2756406.2757741},
	doi = {10.1145/2756406.2757741},
	acmid = {2757741},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {captchas, crowdsourcing, debugging, inter-rater agreement, labeling, relevance judgment, worker reliability},
} 


@inproceedings{Kazai:2016,
	author = {Kazai, Gabriella and Zitouni, Imed},
	title = {Quality Management in Crowdsourcing Using Gold Judges Behavior},
	booktitle = {Proceedings of the Ninth ACM International Conference on Web Search and Data Mining},
	series = {WSDM '16},
	year = {2016},
	isbn = {978-1-4503-3716-8},
	location = {San Francisco, California, USA},
	pages = {267--276},
	numpages = {10},
	url = {http://doi.acm.org/10.1145/2835776.2835835},
	doi = {10.1145/2835776.2835835},
	acmid = {2835835},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {experimentation, measurement},
} 

@inproceedings{Eickhoff:2012,
	author = {Eickhoff, Carsten and Harris, Christopher G. and de Vries, Arjen P. and Srinivasan, Padmini},
	title = {Quality Through Flow and Immersion: Gamifying Crowdsourced Relevance Assessments},
	booktitle = {Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	series = {SIGIR '12},
	year = {2012},
	isbn = {978-1-4503-1472-5},
	location = {Portland, Oregon, USA},
	pages = {871--880},
	numpages = {10},
	url = {http://doi.acm.org/10.1145/2348283.2348400},
	doi = {10.1145/2348283.2348400},
	acmid = {2348400},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {clustering, crowdsourcing, gamification, relevance assessments, serious games},
} 


@inproceedings{Megorskaya2015,
	author = {Megorskaya, Olga and Kukushkin, Vladimir and Serdyukov, Pavel},
	title = {On the Relation Between Assessor's Agreement and Accuracy in Gamified Relevance Assessment},
	booktitle = {Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	series = {SIGIR '15},
	year = {2015},
	isbn = {978-1-4503-3621-5},
	location = {Santiago, Chile},
	pages = {605--614},
	numpages = {10},
	url = {http://doi.acm.org/10.1145/2766462.2767727},
	doi = {10.1145/2766462.2767727},
	acmid = {2767727},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {agreement vs. accuracy, judgments collection workflow, relevance labels},
}


@inproceedings{Davtyan2015,
	author = {Davtyan, Martin and Eickhoff, Carsten and Hofmann, Thomas},
	title = {Exploiting Document Content for Efficient Aggregation of Crowdsourcing Votes},
	booktitle = {Proceedings of the 24th ACM International on Conference on Information and Knowledge Management},
	series = {CIKM '15},
	year = {2015},
	isbn = {978-1-4503-3794-6},
	location = {Melbourne, Australia},
	pages = {783--790},
	numpages = {8},
	url = {http://doi.acm.org/10.1145/2806416.2806460},
	doi = {10.1145/2806416.2806460},
	acmid = {2806460},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {clustering hypothesis, crowdsourcing, relevance assessment},
}

@inproceedings{Rokicki:2014,
	author = {Rokicki, Markus and Chelaru, Sergiu and Zerr, Sergej and Siersdorfer, Stefan},
	title = {Competitive Game Designs for Improving the Cost Effectiveness of Crowdsourcing},
	booktitle = {Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management},
	series = {CIKM '14},
	year = {2014},
	isbn = {978-1-4503-2598-1},
	location = {Shanghai, China},
	pages = {1469--1478},
	numpages = {10},
	url = {http://doi.acm.org/10.1145/2661829.2661946},
	doi = {10.1145/2661829.2661946},
	acmid = {2661946},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {competitions, crowdsourcing, lotteries, reward schemes},
} 

@inproceedings{Blanco:2011,
	author = {Blanco, Roi and Halpin, Harry and Herzig, Daniel M. and Mika, Peter and Pound, Jeffrey and Thompson, Henry S. and Tran Duc, Thanh},
	title = {Repeatable and Reliable Search System Evaluation Using Crowdsourcing},
	booktitle = {Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	series = {SIGIR '11},
	year = {2011},
	isbn = {978-1-4503-0757-4},
	location = {Beijing, China},
	pages = {923--932},
	numpages = {10},
	url = {http://doi.acm.org/10.1145/2009916.2010039},
	doi = {10.1145/2009916.2010039},
	acmid = {2010039},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {crowdsourcing, evaluation, retrieval, search engines},
} 
